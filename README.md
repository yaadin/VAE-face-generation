## Variational Autoencoder (VAE) — Face Generation

# Dataset

The model is trained on the CelebA dataset, consisting of celebrity face images.  

Images are center-cropped, resized to a fixed spatial resolution, normalized to [-1, 1], and loaded using PyTorch DataLoader

# Model

The architecture is a Variational Autoencoder (VAE) composed of:

- **Encoder**: Convolutional layers and MLP that map input images to latent distribution μ, log(σ²)
  
- **Latent Space**: Gaussian normal
  
- **Decoder**: Transposed convolutional layers to reconstruct images
  
Reparameterization is used to enable backpropagation through the stochastic latent space.

# Training

The model is trained using a combined loss:

- **Reconstruction loss** MSE

- **KL divergence** to push the distribution toward a Gaussian

Training is performed on GPU with mini-batches, Adam optimizer, and epoch loss tracking.

# Inference

After training, new faces are generated by:

1. Sampling latent vectors from a standard normal distribution

2. Passing them through the decoder
   
3. Rescaling outputs to image space for visualization

This allows the model to generate previously unseen faces.
